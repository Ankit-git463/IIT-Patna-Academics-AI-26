{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG19 Feature Extraction and Classification for CIFAR-10\n",
    "\n",
    "This notebook implements feature extraction using VGG19 and trains various classifiers on the CIFAR-10 dataset. It uses disk-based storage to handle the full dataset efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG19\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.preprocessing import StandardScaler, LabelBinarizer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "import pandas as pd\n",
    "from time import time\n",
    "import gc\n",
    "import h5py\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Feature Extractor Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class FeatureExtractor:\n",
    "    def __init__(self, batch_size=32, feature_store_path='./features'):\n",
    "        self.batch_size = batch_size\n",
    "        self.feature_store_path = feature_store_path\n",
    "        os.makedirs(feature_store_path, exist_ok=True)\n",
    "        \n",
    "        print(\"Loading VGG19 model...\")\n",
    "        self.base_model = VGG19(\n",
    "            weights='imagenet', \n",
    "            include_top=False, \n",
    "            input_shape=(32, 32, 3)\n",
    "        )\n",
    "        \n",
    "        self.layers_to_extract = [\n",
    "            'block3_conv3',\n",
    "            'block4_conv3',\n",
    "            'block5_conv3'\n",
    "        ]\n",
    "        \n",
    "        print(\"Creating feature extractors...\")\n",
    "        self.feature_extractors = {}\n",
    "        for layer in self.layers_to_extract:\n",
    "            self.feature_extractors[layer] = Model(\n",
    "                inputs=self.base_model.input,\n",
    "                outputs=self.base_model.get_layer(layer).output\n",
    "            )\n",
    "    \n",
    "    def process_dataset(self, x_data, y_data, dataset_type='train'):\n",
    "        \"\"\"Process dataset and save features to disk\"\"\"\n",
    "        num_samples = len(x_data)\n",
    "        \n",
    "        h5_path = os.path.join(\n",
    "            self.feature_store_path, \n",
    "            f'features_{dataset_type}.h5'\n",
    "        )\n",
    "        \n",
    "        with h5py.File(h5_path, 'w') as h5f:\n",
    "            h5f.create_dataset('labels', data=y_data)\n",
    "            \n",
    "            for layer_name in self.layers_to_extract:\n",
    "                print(f\"Processing layer {layer_name} for {dataset_type}\")\n",
    "                extractor = self.feature_extractors[layer_name]\n",
    "                \n",
    "                for start_idx in range(0, num_samples, self.batch_size):\n",
    "                    end_idx = min(start_idx + self.batch_size, num_samples)\n",
    "                    batch_data = x_data[start_idx:end_idx]\n",
    "                    \n",
    "                    batch_features = extractor.predict(batch_data, verbose=0)\n",
    "                    batch_features = np.mean(batch_features, axis=(1, 2))\n",
    "                    \n",
    "                    if start_idx == 0:\n",
    "                        feature_shape = (num_samples, batch_features.shape[1])\n",
    "                        h5f.create_dataset(\n",
    "                            f'features_{layer_name}',\n",
    "                            shape=feature_shape,\n",
    "                            dtype='float32'\n",
    "                        )\n",
    "                    \n",
    "                    h5f[f'features_{layer_name}'][start_idx:end_idx] = batch_features\n",
    "                    \n",
    "                    del batch_features\n",
    "                    gc.collect()\n",
    "                    \n",
    "                    print(f\"Processed {end_idx}/{num_samples} samples\", end='\\r')\n",
    "                print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def train_evaluate_model(h5_train_path, h5_test_path, layer_name, classifier):\n",
    "    \"\"\"Train and evaluate model using stored features\"\"\"\n",
    "    print(f\"Training model on features from {layer_name}...\")\n",
    "    \n",
    "    with h5py.File(h5_train_path, 'r') as h5f:\n",
    "        feature_dset = h5f[f'features_{layer_name}']\n",
    "        label_dset = h5f['labels']\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        \n",
    "        for start_idx in range(0, len(feature_dset), 1000):\n",
    "            end_idx = min(start_idx + 1000, len(feature_dset))\n",
    "            scaler.partial_fit(feature_dset[start_idx:end_idx])\n",
    "        \n",
    "        X_train = feature_dset[:]\n",
    "        y_train = label_dset[:]\n",
    "        X_train_scaled = scaler.transform(X_train)\n",
    "        \n",
    "        classifier.fit(X_train_scaled, y_train.argmax(axis=1))\n",
    "    \n",
    "    print(\"Evaluating model...\")\n",
    "    with h5py.File(h5_test_path, 'r') as h5f:\n",
    "        X_test = h5f[f'features_{layer_name}'][:]\n",
    "        y_test = h5f['labels'][:]\n",
    "        \n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        y_pred = classifier.predict(X_test_scaled)\n",
    "        \n",
    "        accuracy = accuracy_score(y_test.argmax(axis=1), y_pred)\n",
    "        report = classification_report(y_test.argmax(axis=1), y_pred, \n",
    "                                    output_dict=True)\n",
    "        \n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': report['weighted avg']['precision'],\n",
    "            'recall': report['weighted avg']['recall'],\n",
    "            'f1': report['weighted avg']['f1-score']\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"Loading CIFAR-10 dataset...\")\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Normalize pixel values\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "lb = LabelBinarizer()\n",
    "y_train = lb.fit_transform(y_train)\n",
    "y_test = lb.transform(y_test)\n",
    "\n",
    "print(\"Training set shape:\", x_train.shape)\n",
    "print(\"Test set shape:\", x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Extract Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize feature extractor\n",
    "extractor = FeatureExtractor(batch_size=32)\n",
    "\n",
    "# Process and save features\n",
    "print(\"\\nProcessing training data...\")\n",
    "extractor.process_dataset(x_train, y_train, 'train')\n",
    "\n",
    "print(\"\\nProcessing test data...\")\n",
    "extractor.process_dataset(x_test, y_test, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train and Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define classifiers\n",
    "classifiers = {\n",
    "    'LogisticRegression': LogisticRegression(\n",
    "        solver='sag',\n",
    "        max_iter=100,\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    'LinearSVC': LinearSVC(\n",
    "        dual=False,\n",
    "        max_iter=1000\n",
    "    )\n",
    "}\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "\n",
    "# Train and evaluate models\n",
    "for layer_name in extractor.layers_to_extract:\n",
    "    for clf_name, clf in classifiers.items():\n",
    "        print(f\"\\nEvaluating {clf_name} on {layer_name}\")\n",
    "        \n",
    "        metrics = train_evaluate_model(\n",
    "            os.path.join(extractor.feature_store_path, 'features_train.h5'),\n",
    "            os.path.join(extractor.feature_store_path, 'features_test.h5'),\n",
    "            layer_name,\n",
    "            clf\n",
    "        )\n",
    "        \n",
    "        results.append({\n",
    "            'Layer': layer_name,\n",
    "            'Classifier': clf_name,\n",
    "            **metrics\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nResults Summary:\")\n",
    "display(results_df.round(4))\n",
    "\n",
    "# Find best combination\n",
    "best_idx = results_df['accuracy'].idxmax()\n",
    "best_combination = results_df.iloc[best_idx]\n",
    "\n",
    "print(\"\\nBest Combination:\")\n",
    "print(f\"Layer: {best_combination['Layer']}\")\n",
    "print(f\"Classifier: {best_combination['Classifier']}\")\n",
    "print(f\"Accuracy: {best_combination['accuracy']:.4f}\")\n",
    "print(f\"F1 Score: {best_combination['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Clean Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Optional: Remove feature files to free up disk space\n",
    "import shutil\n",
    "if os.path.exists('./features'):\n",
    "    shutil.rmtree('./features')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
