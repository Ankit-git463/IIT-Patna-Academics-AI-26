{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3RfsjSNj4w_p"
      },
      "outputs": [],
      "source": [
        "# Upload .tar file for Mahout and Spark on Google Drive\n",
        "# Mount same Drive account to Google Collab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "d8t_E5_65N_6"
      },
      "outputs": [],
      "source": [
        "!apt-get update -qq\n",
        "!apt-get install -y -qq openjdk-8-jdk-headless wget tar\n",
        "!apt install -y pigz > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ls0zh8F96p0_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import subprocess\n",
        "from google.colab import drive\n",
        "\n",
        "# drive_tar here is path for spark tar file in Google drive (adjust accordingly)\n",
        "drive_tar = '/content/drive/MyDrive/spark-2.2.0-bin-hadoop2.7.tgz'  # change this\n",
        "local_tar = '/content/spark-2.2.0-bin-hadoop2.7.tgz'\n",
        "extract_dir = '/content/spark'\n",
        "\n",
        "# --- Copy from Drive  ---\n",
        "os.makedirs(extract_dir, exist_ok=True)\n",
        "print(\"Copying from Drive to Colab...\")\n",
        "subprocess.run([\"cp\", drive_tar, local_tar])\n",
        "\n",
        "# --- Step 1: Multi-threaded decompression ---\n",
        "print(\"Decompressing with 2 threads using pigz...\")\n",
        "subprocess.run([\"pigz\", \"-d\", \"-p\", \"2\", local_tar])\n",
        "\n",
        "# --- Step 2: Extract .tar ---\n",
        "tar_file = local_tar.replace(\".gz\", \"\")\n",
        "print(f\"Extracting {tar_file} to {extract_dir}...\")\n",
        "subprocess.run([\"tar\", \"-xf\", tar_file, \"-C\", extract_dir])\n",
        "\n",
        "print(f\"\\nâœ… Extraction complete! Files are in: {extract_dir}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Basic imports for Shortest Paths\n",
        "import org.apache.spark._;\n",
        "import org.apache.spark.graphx._;\n",
        "import org.apache.spark.rdd.RDD;\n",
        "import org.apache.spark.graphx.lib.ShortestPaths\n",
        "\n",
        "#Create Vertex RDD (String type)\n",
        "  val vertices: RDD[(VertexId, String)] = sc.parallelize(Seq(\n",
        "    (1L, \"A\"),\n",
        "    (2L, \"B\"),\n",
        "    (3L, \"C\"),\n",
        "    (4L, \"D\"),\n",
        "    (5L, \"E\"),\n",
        "    (6L, \"F\"),\n",
        "    (7L, \"G\"),\n",
        "    (8L, \"H\"),\n",
        "    (9L, \"I\"),\n",
        "    (10L, \"J\")\n",
        "  ))\n",
        "\n",
        "# Create Edge RDD (Integer type  for weighted graph)\n",
        "  val edges: RDD[Edge[Int]] = sc.parallelize(Seq(\n",
        "    Edge(1L, 2L, 1), Edge(1L, 3L, 1),\n",
        "    Edge(2L, 4L, 1), Edge(2L, 5L, 1),\n",
        "    Edge(3L, 6L, 1), Edge(3L, 7L, 1),\n",
        "    Edge(4L, 8L, 1), Edge(5L, 8L, 1),\n",
        "    Edge(6L, 9L, 1), Edge(7L, 9L, 1),\n",
        "    Edge(8L, 10L, 1), Edge(9L, 10L, 1),\n",
        "    Edge(5L, 6L, 1), Edge(7L, 4L, 1),\n",
        "    Edge(9L, 1L, 1)\n",
        "  ))\n",
        "\n",
        "#Create Graph\n",
        "   val graph = Graph(vertices, edges)\n",
        "\n",
        "#Create Set of Vertex as target\n",
        "   val landmarks = Seq(1L, 5L)\n",
        "\n",
        "#Create graph with shortest distance mapping as vertex attributes\n",
        "   val results = ShortestPaths.run(graph, landmarks)\n",
        "\n",
        "#Display Result\n",
        "   results.vertices.collect().foreach(println)\n",
        "\n",
        "#Save Graphs to view using networkx\n",
        "  graph.vertices\n",
        "    .map { case (id, attr) => s\"$id,$attr\" }\n",
        "    .coalesce(1)\n",
        "    .saveAsTextFile(\"/content/vertices_csv\")\n",
        "\n",
        "  graph.edges\n",
        "    .map(e => s\"${e.srcId},${e.dstId},${e.attr}\")\n",
        "    .coalesce(1)\n",
        "    .saveAsTextFile(\"/content/edges_csv\")\n"
      ],
      "metadata": {
        "id": "MTaJZ1YCbKxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8yhNRIOtHOCL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "#Run command to open scala shell\n",
        "os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-8-openjdk-amd64'\n",
        "os.environ['SPARK_HOME'] = '/content/spark/spark-2.2.0-bin-hadoop2.7'\n",
        "os.environ['PATH'] += f\":{os.environ['SPARK_HOME']}/bin\"\n",
        "\n",
        "!spark-shell --master local[*] --driver-memory 2g\n",
        "\n",
        "\n",
        "#If it does not work run last cell  again\n",
        "# Run previous cell as practice example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YpIEANO3KmXG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load CSV files\n",
        "vertices = pd.read_csv(\"/content/vertices_csv/part-00000\", names=[\"id\", \"name\"])\n",
        "edges = pd.read_csv(\"/content/edges_csv/part-00000\", names=[\"src\", \"dst\", \"relation\"])\n",
        "\n",
        "# Create a directed graph (or use nx.Graph() for undirected)\n",
        "G = nx.DiGraph()\n",
        "\n",
        "# Add nodes with attributes\n",
        "for _, row in vertices.iterrows():\n",
        "    G.add_node(row['id'], name=row['name'])\n",
        "\n",
        "# Add edges with attributes\n",
        "for _, row in edges.iterrows():\n",
        "    G.add_edge(row['src'], row['dst'], relation=row['relation'])\n",
        "\n",
        "# Draw the graph\n",
        "pos = nx.spring_layout(G)\n",
        "nx.draw(G, pos, with_labels=True, labels=nx.get_node_attributes(G, 'name'))\n",
        "edge_labels = nx.get_edge_attributes(G, 'relation')\n",
        "nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ADIJr6RMDIV"
      },
      "outputs": [],
      "source": [
        "#Delete a folders content as neeeded and all its contents\n",
        "!rm -rf /content/vertices.csv\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}