{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3RfsjSNj4w_p"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.13.9' requires the ipykernel package.\n",
            "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
            "\u001b[1;31mOr install 'ipykernel' using the command: 'c:/Users/ASUS/AppData/Local/Microsoft/WindowsApps/python3.13.exe -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "# Upload .tar file for Mahout and Spark on Google Drive\n",
        "# Mount same Drive account to Google Collab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "d8t_E5_65N_6"
      },
      "outputs": [],
      "source": [
        "!apt-get update -qq\n",
        "!apt-get install -y -qq openjdk-8-jdk-headless wget tar\n",
        "!apt install -y pigz > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ls0zh8F96p0_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import subprocess\n",
        "from google.colab import drive\n",
        "\n",
        "# drive_tar here is path for spark tar file in Google drive (adjust accordingly)\n",
        "drive_tar = '/content/drive/MyDrive/spark-2.2.0-bin-hadoop2.7.tgz'  # change this\n",
        "local_tar = '/content/spark-2.2.0-bin-hadoop2.7.tgz'\n",
        "extract_dir = '/content/spark'\n",
        "\n",
        "# --- Copy from Drive  ---\n",
        "os.makedirs(extract_dir, exist_ok=True)\n",
        "print(\"Copying from Drive to Colab...\")\n",
        "subprocess.run([\"cp\", drive_tar, local_tar])\n",
        "\n",
        "# --- Step 1: Multi-threaded decompression ---\n",
        "print(\"Decompressing with 2 threads using pigz...\")\n",
        "subprocess.run([\"pigz\", \"-d\", \"-p\", \"2\", local_tar])\n",
        "\n",
        "# --- Step 2: Extract .tar ---\n",
        "tar_file = local_tar.replace(\".gz\", \"\")\n",
        "print(f\"Extracting {tar_file} to {extract_dir}...\")\n",
        "subprocess.run([\"tar\", \"-xf\", tar_file, \"-C\", extract_dir])\n",
        "\n",
        "print(f\"\\nâœ… Extraction complete! Files are in: {extract_dir}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1dtKIMCSKPuP"
      },
      "outputs": [],
      "source": [
        "# Lab Example\n",
        "# Basic Imports\n",
        "import org.apache.spark.graphx._\n",
        "import org.apache.spark.rdd.RDD\n",
        "\n",
        "# Create 10 vertices (id, name)\n",
        "val vertices: RDD[(VertexId, String)] = sc.parallelize(Seq(\n",
        "  (1L, \"Alice\"),\n",
        "  (2L, \"Bob\"),\n",
        "  (3L, \"Charlie\"),\n",
        "  (4L, \"David\"),\n",
        "  (5L, \"Eve\"),\n",
        "  (6L, \"Frank\"),\n",
        "  (7L, \"Grace\"),\n",
        "  (8L, \"Hannah\"),\n",
        "  (9L, \"Ivy\"),\n",
        "  (10L, \"Jack\")\n",
        "))\n",
        "\n",
        "# Create some edges (srcId, dstId, relationship)\n",
        "val edges: RDD[Edge[String]] = sc.parallelize(Seq(\n",
        "  Edge(1L, 2L, \"friend\"),\n",
        "  Edge(2L, 3L, \"friend\"),\n",
        "  Edge(3L, 4L, \"follow\"),\n",
        "  Edge(4L, 5L, \"friend\"),\n",
        "  Edge(5L, 6L, \"follow\"),\n",
        "  Edge(6L, 7L, \"friend\"),\n",
        "  Edge(7L, 8L, \"follow\"),\n",
        "  Edge(8L, 9L, \"friend\"),\n",
        "  Edge(9L, 10L, \"follow\"),\n",
        "  Edge(10L, 1L, \"friend\"),\n",
        "  Edge(1L, 5L, \"friend\"),\n",
        "  Edge(2L, 6L, \"follow\")\n",
        "))\n",
        "\n",
        "# Build the Graph\n",
        "val graph = Graph(vertices, edges)\n",
        "\n",
        "# Compute the degree of each vertex (in + out)\n",
        "val degrees: VertexRDD[Int] = graph.degrees\n",
        "\n",
        "val highDegreeVertices = degrees.filter { case (id, deg) => deg >= 3 }\n",
        "\n",
        "val highDegreeIDs = highDegreeVertices.map(_._1).collect()\n",
        "\n",
        "val highDegreeIDs_b = sc.broadcast(highDegreeIDs)\n",
        "\n",
        "val subgraph = graph.subgraph(\n",
        "  vpred = (id, attr) => highDegreeIDs_b.value.contains(id)\n",
        ")\n",
        "\n",
        "val inSub = subgraph.inDegrees\n",
        "\n",
        "val outSub = subgraph.outDegrees\n",
        "\n",
        "\n",
        "# Save vertices and edges as CSV for Python / NetworkX\n",
        "# Coalesce to 1 partition so we get a single CSV file\n",
        "graph.vertices\n",
        "  .map { case (id, attr) => s\"$id,$attr\" }\n",
        "  .coalesce(1)\n",
        "  .saveAsTextFile(\"/content/vertices_csv\")\n",
        "\n",
        "graph.edges\n",
        "  .map(e => s\"${e.srcId},${e.dstId},${e.attr}\")\n",
        "  .coalesce(1)\n",
        "  .saveAsTextFile(\"/content/edges_csv\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8yhNRIOtHOCL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "#Run command to open scala shell\n",
        "os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-8-openjdk-amd64'\n",
        "os.environ['SPARK_HOME'] = '/content/spark/spark-2.2.0-bin-hadoop2.7'\n",
        "os.environ['PATH'] += f\":{os.environ['SPARK_HOME']}/bin\"\n",
        "\n",
        "!spark-shell --master local[*] --driver-memory 2g\n",
        "\n",
        "\n",
        "#If it does not work run last cell  again\n",
        "# Run previous cell as practice example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "4ADIJr6RMDIV"
      },
      "outputs": [],
      "source": [
        "#Delete folder /content/vertices_csv and all its contents\n",
        "!rm -rf /content/vertices.csv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YpIEANO3KmXG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load CSV files\n",
        "vertices = pd.read_csv(\"/content/vertices_csv/part-00000\", names=[\"id\", \"name\"])\n",
        "edges = pd.read_csv(\"/content/edges_csv/part-00000\", names=[\"src\", \"dst\", \"relation\"])\n",
        "\n",
        "# Create a directed graph (or use nx.Graph() for undirected)\n",
        "G = nx.DiGraph()\n",
        "\n",
        "# Add nodes with attributes\n",
        "for _, row in vertices.iterrows():\n",
        "    G.add_node(row['id'], name=row['name'])\n",
        "\n",
        "# Add edges with attributes\n",
        "for _, row in edges.iterrows():\n",
        "    G.add_edge(row['src'], row['dst'], relation=row['relation'])\n",
        "\n",
        "# Draw the graph\n",
        "pos = nx.spring_layout(G)\n",
        "nx.draw(G, pos, with_labels=True, labels=nx.get_node_attributes(G, 'name'))\n",
        "edge_labels = nx.get_edge_attributes(G, 'relation')\n",
        "nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels)\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
